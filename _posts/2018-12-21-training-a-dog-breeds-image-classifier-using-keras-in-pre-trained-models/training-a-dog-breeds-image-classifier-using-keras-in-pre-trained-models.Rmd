---
title: "Training a Dog Breed Image Classifier using Keras and Pre-Trained Models"
description: |
  In this post we train Dog Breed Image Classifier in R using the Keras package and pre-trained models. The code could be easily adapted to similar problems.
author:
  - name: Paulo Felipe Alencar
    url: https://github.com/paulofelipe
date: 12-21-2018
output:
  radix::radix_article:
    self_contained: false
draft: true
categories:
  - Deep Learning
  - Kaggle
  - Keras
  - Computer Vision
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

In this post, we will create a simple dog breeds classifierin R. Although the number of images is relatively small (10222 images), it is possible to use pre-trained models and extract features that will allow us to train the classifier.

The Keras package will be used. Our classifier has reached %%% in validation.

In addition, we also try to implement some callbacks that are taught in the [fastai course](https://course.fast.ai/).

## Packages

These are the packages that will be used in this post:

```{r, echo = TRUE}
library(keras)
library(tidyverse)
library(rsample)
library(glue)
library(magick)
library(patchwork)
```

## Data

The data for training the classifier is available on [Kaggle] (www.kaggle.com). You can download directly from the [competition page] (https://www.kaggle.com/c/dog-breed-identification). If you have the Kaggle API python library configured on your computer, you can get the data using the following command:

```{bash, echo = TRUE, eval = FALSE}
kaggle competitions download -c dog-breed-identification
```

Once you have downloaded the data and the file has been unziped, the folder will have the following files/folders:

```{r, echo = TRUE}
# Path to the main folder
data_path <- 'dog_breeds'
list.files(data_path)
```

```{r, echo=TRUE}
labels <- read_csv(file.path(data_path, 'labels.csv')) 
glimpse(labels)
```

The number of classes is equal to 120.

```{r, echo=TRUE}
length(unique(labels$breed))
```

Let's see some images:

```{r, echo = TRUE}
show_image <- function(id, breed){
  file.path(data_path, "train", glue("{id}.jpg")) %>% 
    image_read() %>% 
    image_ggplot() +
    labs(title = breed) +
    theme(
      plot.title = element_text(hjust = 0.5)
    )
}

set.seed(3011)
labels %>% 
  sample_n(4) %>% 
  pmap(., show_image) %>% 
  wrap_plots()

```

## Train and Validation Split

The code below creates two data.frames, one for training and another for validation. We will use 90% of the images that are in the `train` folder to train the model. The remaining images will be used to validate the model.

```{r, echo=TRUE}
set.seed(93891)
labels <- initial_split(labels, prop = 0.9,strata = "breed")

train_data <- training(labels)

valid_data <- testing(labels)
```

## Extracting features from pre-trained models

To train an image classifier, there are three basic strategies:

* Train a new model from scratch;

* Use a pre-trained model and do a finetuning on some model layers;

* Extract the features from the convolutional base of the model and train a new classifier using these features.

We will use the third approach. This approach is best suited for when the training base is small (small number of images per class) and it approaches to some degree the images that were used in the training of the model that we will use to extract the features ([see here] : //www.kdnuggets.com/2018/12/solve-image-classification-problem-quickly-easily.html)). As the model we will use has been trained using images that contained pictures of dogs, we can assume that the pre-trained model can serve as the basis for our new classifier.

We will combine the features extracted from two models: *Xception* and *Inception-ResNet v2*. Both models were trained with images from the ImageNet database. To create objects with these models, you can execute the following code:

```{r, echo=TRUE}
xception_model <- application_xception(
  include_top = FALSE,
  weights = 'imagenet',
  input_shape = c(299, 299, 3),
  pooling = 'avg'
)

inception_model <- application_inception_resnet_v2(
  include_top = FALSE,
  weights = 'imagenet',
  input_shape = c(299, 299, 3),
  pooling = 'avg'
)
```

In the next step, we will create the image generators. That is, objects that provide batches of images that will be preprocessed and used as input in the base model and the output are the features that will be used as inputs in the breed classifier that we will train.

First, we will create the generators. For the training image generator, we will use a technique called "data augmentation", which consists in altering the original images in an attempt to increase the number of observations that will be used in the training. This helps our model to generalize better, since we have few images for each class.

```{r}
batch_size <- 16L
image_size <- c(299L, 299L)
classes <- sort(unique(train_data$breed))

img_gen_train <- image_data_generator(
  rotation_range = 30,
  zoom_range = 0.2,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  horizontal_flip = TRUE
)

train_gen <- img_gen_train$flow_from_dataframe(
  dataframe = train_data,
  directory = 'dog_breeds/train/',
  x_col = "id",
  y_col = "breed",
  classes = classes,
  has_ext = FALSE,
  target_size = image_size,
  batch_size = batch_size
)
```

We do the same for validation, but we will not "augment" the data:

```{r}
img_gen_valid <- image_data_generator()

valid_gen <- img_gen_valid$flow_from_dataframe(
  dataframe = valid_data,
  directory = 'dog_breeds/train/',
  x_col = "id",
  y_col = "breed",
  classes = classes,
  has_ext = FALSE,
  target_size = c(299L,299L),
  batch_size = batch_size
)
```

Now let's define the function that extracts the features. This function has two arguments: an image generator and the number of batches of images that will be used. So if the size of the batch is 16 and we define 10 batches, data will be extracted for 160 images. The function returns an input matrix and an array of targets.

```{r}
extract_features <- function(image_generator, num_batches = 10){
  
  xception_out <- dim(xception_model$output)[[2]]
  inception_out <- dim(inception_model$output)[[2]]
  n_features <- xception_out + inception_out
  
  x <- matrix(NA, num_batches * batch_size, n_features)
  y <- matrix(NA, num_batches * batch_size, length(classes))
  
  for(i in 1:num_batches){
    
    g <- generator_next(image_generator)
    
    x1 <- g[[1]] %>% 
      xception_preprocess_input() %>% 
      predict(xception_model, .)
    
    x2 <- g[[1]] %>% 
      inception_resnet_v2_preprocess_input() %>% 
      predict(inception_model, .)
    
    idx <- (1 + (i - 1) * batch_size):(batch_size + (i - 1) * batch_size)
    idx <- idx[1:nrow(x1)]
    x[idx,] <- cbind(x1, x2)
    y[idx,] <- g[[2]]
    
  }
  
  x <- x[complete.cases(x),]
  y <- y[complete.cases(y),]
  return(list(x = x, y = y))
}
```

First let's extract the features from the training images. We are going to create a training data for 18400 images. Our training set only has `r nrow (train_data)` 9200 images, but since we are using transformations of the original images, no image will be exactly repeated. Of course the ideal would be to get a larger base of really different images.

```{r}
data_tr <- extract_features(train_gen,
                            num_batches = ceiling(train_gen$n/batch_size * 2))

xtrain <- data_tr$x
ytrain <- data_tr$y
rm(data_tr)
```

```{r}
data_vl <- extract_features(valid_gen, num_batches = ceiling(valid_gen$n/batch_size))
xvalid <- data_vl$x
yvalid <- data_vl$y
rm(data_vl)
```


## Training the Classifier

```{r, echo=FALSE, eval=TRUE}
lr_finder <- R6::R6Class("lrFinder",
                         inherit = KerasCallback,
                         public = list(
                           min_lr = NULL,
                           max_lr = NULL,
                           beta = NULL,
                           step_size = NULL,
                           iteration = 0,
                           lr_mult = 0,
                           best_loss = 0,
                           avg_loss = 0,
                           history = data.frame(),
                           initialize = function(min_lr = 1e-5,
                                                 max_lr = 1e-2,
                                                 step_size = NULL,
                                                 beta = 0.98){
                             self$min_lr <- min_lr
                             self$max_lr <- max_lr
                             self$step_size <- step_size
                             self$beta <- beta
                           },
                           lr_plot = function(){
                             ggplot(self$history, aes(x = lr, y = avg_loss)) +
                               geom_line() +
                               scale_x_log10() +
                               labs(
                                 y = "Average Loss",
                                 x = "Learning Rate (log 10)",
                                 title = "Learning Rate Finder"
                               ) +
                               theme_minimal()
                           },
                           clr = function(){
                             self$lr_mult <- (self$max_lr / self$min_lr)^(1/self$step_size)
                             self$lr_mult
                           },
                           on_train_begin = function(logs = list()){
                             k_set_value(self$model$optimizer$lr, self$min_lr)
                           },
                           on_batch_end = function(batch, logs = list()){
                             self$iteration <- self$iteration + 1
                             
                             loss <- logs[["loss"]]
                             self$avg_loss <- self$beta * self$avg_loss + (1-self$beta) * loss
                             smoothed_loss <- self$avg_loss/(1-self$beta^self$iteration)
                             
                             if(self$iteration > 1 & smoothed_loss > self$best_loss * 4){
                               self$model$stop_training <- TRUE
                             }
                             
                             if(smoothed_loss < self$best_loss | self$iteration == 1){
                               self$best_loss <- smoothed_loss
                             }
                             
                             lr = k_get_value(self$model$optimizer$lr)*self$clr()
                             
                             history_tmp <- data.frame(
                               lr = k_get_value(self$model$optimizer$lr),
                               iterations = self$iteration,
                               avg_loss = smoothed_loss
                             )
                             
                             self$history <- rbind(self$history, history_tmp)
                             
                             k_set_value(self$model$optimizer$lr, lr)
                             
                           }
                         )
)

one_cycle_learn <- R6::R6Class(
  "OneCycleLearning",
  
  inherit = KerasCallback,
  
  public = list(
    max_lr = NULL,
    min_lr = NULL,
    div_factor = NULL,
    moms = NULL,
    pct_start = NULL,
    batch_size = NULL,
    n_train = NULL,
    epochs = NULL,
    max_mom = NULL,
    min_mom = NULL,
    a1 = NULL,
    a2 = NULL,
    clr_iterations = 0,
    trn_iterations = 0,
    history = data.frame(),
    initialize = function(max_lr = 0.01,
                          div_factor= 25,
                          moms = c(0.85, 0.95),
                          pct_start = 0.3,
                          n_train = NULL,
                          epochs = 1,
                          batch_size = 16){
      
      self$max_lr <- max_lr
      self$min_lr <- max_lr/div_factor
      self$max_mom <- moms[2]
      self$min_mom <- moms[1]
      self$pct_start = pct_start
      self$n_train <- n_train
      self$batch_size <- batch_size
      
      n <- n_train/batch_size * epochs
      self$a1 <- round(n * pct_start)
      self$a2 <- n - self$a1
      
      self$clr_iterations = 0
      self$trn_iterations = 0
      
    },
    clr = function(){
      
      if(self$clr_iterations <= self$a1){
        return(
          self$min_lr + (self$max_lr - self$min_lr) * self$clr_iterations/self$a1
        )
      }
      
      if(self$clr_iterations > self$a1){
        return(
          self$min_lr/1e4 + 0.5 * (self$max_lr - self$min_lr/1e4) * 
            (1 + cos((self$clr_iterations - self$a1)/self$a2 * pi))
        )
      }
    },
    cm = function(){
      if(self$clr_iterations <= self$a1){
        return(
          self$max_mom - (self$max_mom - self$min_mom) * self$clr_iterations/self$a1
        )
      }
      
      if(self$clr_iterations > self$a1){
        return(
          self$max_mom - 0.5 * (self$max_mom - self$min_mom) * 
            (1 + cos((self$clr_iterations - self$a1)/self$a2 * pi))
        )
      }
    },
    on_train_begin = function(logs = list()){
      if(self$clr_iterations == 0){
        k_set_value(self$model$optimizer$lr, self$min_lr)
        k_set_value(self$model$optimizer$beta_1, self$max_mom)
      } else{
        k_set_value(self$model$optimizer$lr, self$clr())
        k_set_value(self$model$optimizer$beta_1, self$cm())
      }
    },
    on_batch_end = function(batch, logs = list()){
      self$trn_iterations = self$trn_iterations + 1
      self$clr_iterations = self$clr_iterations + 1
      
      lr = k_get_value(self$model$optimizer$lr)
      beta_1 = k_get_value(self$model$optimizer$beta_1)
      iterations = self$trn_iterations
      
      history_tmp <- data.frame(
        lr = lr,
        beta_1 = beta_1,
        iterations = iterations
      )
      
      self$history <- rbind(self$history, history_tmp)
      
      k_set_value(self$model$optimizer$lr, self$clr())
      k_set_value(self$model$optimizer$beta_1, self$cm())
      
    }
  )
)

```

After extracting the features, we are going to train a classifier that will have as inputs the `r nrow (xtrain)` variables obtained and as output a vector with probabilities for each one of the 120 classes. The `create_model()` function creates our model.

```{r, echo = TRUE}
create_model <- function(lr = 1e-3){
  model <- keras_model_sequential()
    
  model %>%
    layer_dense(units = 120, activation = 'softmax', input_shape = ncol(xtrain))
  
  model %>%
    compile(optimizer = optimizer_adam(lr = lr),
            loss = 'categorical_crossentropy',
            metrics = "accuracy")
  
  return(model)
}

```

To choose the learning rate, we use a callback that is inspired (that is, attempted to replicate) by the [fastai library] (https://docs.fast.ai/). Let's set a minimum rate and a maximum rate. The "training" will start at the lowest rate and after each batch the learning rate is updated towards the maximum rate. The learning rate will be chosen looking at the results. It is indicated to choose the rate at which the loss function is declining but not at its minimum value.

```{r, echo = TRUE}
model <- create_model()
model

batch_size <- 256
lrf <- lr_finder$new(
  min_lr = 1e-5,
  max_lr = 1,
  step_size = ceiling(nrow(xtrain)/batch_size)
)

history <- model %>%
  fit(
    xtrain, ytrain,
    batch_size = batch_size,
    epochs = 1,
    validation_data = list(xvalid, yvalid),
    callbacks = list(lrf)
  )

lrf$lr_plot()
```

Looking at the figure above, we set the learning rate at 0.001.

In addition, the fastai authors indicate a callback that allows us to fit the model using a method called one cycle policy. As optimizer, [they use the AdamW](https://www.fast.ai/2018/07/02/adam-weight-decay/). However, considering that otpimizer is not readily available in keras, we use the Adam optimizer. For details about the one cycle policy I recommend [this post](https://sgugger.github.io/the-1cycle-policy.html)^[In the AdamW implementation, instead of the cyclical momentum the `beta_1` parameter varies during the training.].


```{r, echo=TRUE}
ocl <- one_cycle_learn$new(
  max_lr = 1e-3,
  div_factor = 25,
  n_train = nrow(xtrain),
  batch_size = batch_size,
  epochs = 10
)
model <- create_model()

history <- model %>%
  fit(
    xtrain, ytrain,
    batch_size = batch_size,
    epochs = 10,
    validation_data = list(xvalid, yvalid),
    verbose = 1,
    callbacks = list(ocl)
  )
```

In the figures below, it is possible to see how the learning rate and the 'beta_1` parameters changed during the training.

```{r, echo=TRUE}
lr_plot <- ocl$history %>% 
  ggplot(aes(x = iterations, y = lr)) +
  geom_line() +
  labs(
    y = "Learning Rate",
    x = "Iteration"
  ) + 
  theme_minimal()

momentum_plot <- ocl$history %>% 
  ggplot(aes(x = iterations, y = beta_1)) +
  geom_line() +
  labs(
    y = "beta_1",
    x = "Iteration"
  ) + 
  theme_minimal()

lr_plot / momentum_plot
```

The history plot:

```{r, echo=TRUE}
plot(history) +
  theme_minimal()
```

The final validation score:

```{r, echo=TRUE}
evaluate(model, xvalid, yvalid)
```


## Predictions on the Test Set

Por fim, vamos gerar as predições para a base de teste. Para usar a função `predict_generator()`, é necessário criar um gerador que preprocesse nossas imagens e retorne matrizes no mesmo formato que foi usado no treinamento do modelo. A função `custom_generator()` faz esse trabalho para a gente.

```{r, echo=TRUE}
test_files <- data.frame(
  id = list.files(file.path(data_path, 'test')),
  breed = NA,
  stringsAsFactors = FALSE
)

test_gen <- image_data_generator()
test_gen <- test_gen$flow_from_dataframe(
  dataframe = test_files,
  directory = file.path(data_path, 'test'),
  x_col = 'id',
  y_col = 'breed',
  has_ext = TRUE,
  target_size = c(299L, 299L),
  shuffle = FALSE,
  class_mode = NULL,
  batch_size = 8L
)

custom_generator <- function(test_gen){
  
  function(){
    data <- generator_next(test_gen)
    input <- data
    
    if(!is.null(test_gen$class_mode)){
      input <- data[[1]]
      y <- data[[2]]
    }
    
    x1 <- input %>%
      xception_preprocess_input() %>%
      predict(xception_model, .)

    x2 <- input %>%
      inception_resnet_v2_preprocess_input() %>%
      predict(inception_model, .)
    
    out <- list(cbind(x1, x2))
    
    if(!is.null(test_gen$class_mode)){
      out[[2]] <- y
    }
    
    out
  }
  
}
``` 

Creating the generator and making predictions:

```{r, echo=TRUE}
test_generator <- custom_generator(test_gen)

test_preds <- predict_generator(model, 
                                test_generator,
                                steps = ceiling(test_gen$n/test_gen$batch_size),
                                verbose = 1) %>% 
  as.data.frame()

names(test_preds) <- classes
test_preds <- test_files %>% 
  mutate(id = str_sub(id, start = 1, end = -5)) %>% 
  select(id) %>% 
  bind_cols(test_preds)

dim(test_preds)
```

## Auxliary Functions

### Learning Rate Finder

```{r, echo=TRUE, eval=FALSE}
lr_finder <- R6::R6Class("lrFinder",
                         inherit = KerasCallback,
                         public = list(
                           min_lr = NULL,
                           max_lr = NULL,
                           beta = NULL,
                           step_size = NULL,
                           iteration = 0,
                           lr_mult = 0,
                           best_loss = 0,
                           avg_loss = 0,
                           history = data.frame(),
                           initialize = function(min_lr = 1e-5,
                                                 max_lr = 1e-2,
                                                 step_size = NULL,
                                                 beta = 0.98){
                             self$min_lr <- min_lr
                             self$max_lr <- max_lr
                             self$step_size <- step_size
                             self$beta <- beta
                           },
                           lr_plot = function(){
                             ggplot(self$history, aes(x = lr, y = avg_loss)) +
                               geom_line() +
                               scale_x_log10() +
                               labs(
                                 y = "Average Loss",
                                 x = "Learning Rate (log 10)",
                                 title = "Learning Rate Finder"
                               ) +
                               theme_minimal()
                           },
                           clr = function(){
                             self$lr_mult <- (self$max_lr / self$min_lr)^(1/self$step_size)
                             self$lr_mult
                           },
                           on_train_begin = function(logs = list()){
                             k_set_value(self$model$optimizer$lr, self$min_lr)
                           },
                           on_batch_end = function(batch, logs = list()){
                             self$iteration <- self$iteration + 1
                             
                             loss <- logs[["loss"]]
                             self$avg_loss <- self$beta * self$avg_loss + (1-self$beta) * loss
                             smoothed_loss <- self$avg_loss/(1-self$beta^self$iteration)
                             
                             if(self$iteration > 1 & smoothed_loss > self$best_loss * 4){
                               self$model$stop_training <- TRUE
                             }
                             
                             if(smoothed_loss < self$best_loss | self$iteration == 1){
                               self$best_loss <- smoothed_loss
                             }
                             
                             lr = k_get_value(self$model$optimizer$lr)*self$clr()
                             
                             history_tmp <- data.frame(
                               lr = k_get_value(self$model$optimizer$lr),
                               iterations = self$iteration,
                               avg_loss = smoothed_loss
                             )
                             
                             self$history <- rbind(self$history, history_tmp)
                             
                             k_set_value(self$model$optimizer$lr, lr)
                             
                           }
                         )
)
```

### One Cycle Learning Policy

```{r, echo=TRUE, eval=FALSE}
one_cycle_learn <- R6::R6Class(
  "OneCycleLearning",
  
  inherit = KerasCallback,
  
  public = list(
    max_lr = NULL,
    min_lr = NULL,
    div_factor = NULL,
    moms = NULL,
    pct_start = NULL,
    batch_size = NULL,
    n_train = NULL,
    epochs = NULL,
    max_mom = NULL,
    min_mom = NULL,
    a1 = NULL,
    a2 = NULL,
    clr_iterations = 0,
    trn_iterations = 0,
    history = data.frame(),
    initialize = function(max_lr = 0.01,
                          div_factor= 25,
                          moms = c(0.85, 0.95),
                          pct_start = 0.3,
                          n_train = NULL,
                          epochs = 1,
                          batch_size = 16){
      
      self$max_lr <- max_lr
      self$min_lr <- max_lr/div_factor
      self$max_mom <- moms[2]
      self$min_mom <- moms[1]
      self$pct_start = pct_start
      self$n_train <- n_train
      self$batch_size <- batch_size
      
      n <- n_train/batch_size * epochs
      self$a1 <- round(n * pct_start)
      self$a2 <- n - self$a1
      
      self$clr_iterations = 0
      self$trn_iterations = 0
      
    },
    clr = function(){
      
      if(self$clr_iterations <= self$a1){
        return(
          self$min_lr + (self$max_lr - self$min_lr) * self$clr_iterations/self$a1
        )
      }
      
      if(self$clr_iterations > self$a1){
        return(
          self$min_lr/1e4 + 0.5 * (self$max_lr - self$min_lr/1e4) * 
            (1 + cos((self$clr_iterations - self$a1)/self$a2 * pi))
        )
      }
    },
    cm = function(){
      if(self$clr_iterations <= self$a1){
        return(
          self$max_mom - (self$max_mom - self$min_mom) * self$clr_iterations/self$a1
        )
      }
      
      if(self$clr_iterations > self$a1){
        return(
          self$max_mom - 0.5 * (self$max_mom - self$min_mom) * 
            (1 + cos((self$clr_iterations - self$a1)/self$a2 * pi))
        )
      }
    },
    on_train_begin = function(logs = list()){
      if(self$clr_iterations == 0){
        k_set_value(self$model$optimizer$lr, self$min_lr)
        k_set_value(self$model$optimizer$beta_1, self$max_mom)
      } else{
        k_set_value(self$model$optimizer$lr, self$clr())
        k_set_value(self$model$optimizer$beta_1, self$cm())
      }
    },
    on_batch_end = function(batch, logs = list()){
      self$trn_iterations = self$trn_iterations + 1
      self$clr_iterations = self$clr_iterations + 1
      
      lr = k_get_value(self$model$optimizer$lr)
      beta_1 = k_get_value(self$model$optimizer$beta_1)
      iterations = self$trn_iterations
      
      history_tmp <- data.frame(
        lr = lr,
        beta_1 = beta_1,
        iterations = iterations
      )
      
      self$history <- rbind(self$history, history_tmp)
      
      k_set_value(self$model$optimizer$lr, self$clr())
      k_set_value(self$model$optimizer$beta_1, self$cm())
      
    }
  )
)

```



